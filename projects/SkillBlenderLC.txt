2:I[5250,["250","static/chunks/250-a256cd67efe5c966.js","101","static/chunks/app/projects/%5Bslug%5D/page-78f26962105b3138.js"],""]
4:I[5613,[],""]
6:I[1778,[],""]
7:I[9463,["699","static/chunks/8e1d74a4-35a4d45ac4ea158f.js","250","static/chunks/250-a256cd67efe5c966.js","374","static/chunks/374-afa9630ac055ee60.js","288","static/chunks/288-dab10db7057b0ce7.js","185","static/chunks/app/layout-46fde1132c555c8e.js"],"GoogleAnalytics"]
8:I[1107,["699","static/chunks/8e1d74a4-35a4d45ac4ea158f.js","250","static/chunks/250-a256cd67efe5c966.js","374","static/chunks/374-afa9630ac055ee60.js","288","static/chunks/288-dab10db7057b0ce7.js","185","static/chunks/app/layout-46fde1132c555c8e.js"],"ThemeProvider"]
9:I[4002,["699","static/chunks/8e1d74a4-35a4d45ac4ea158f.js","250","static/chunks/250-a256cd67efe5c966.js","374","static/chunks/374-afa9630ac055ee60.js","288","static/chunks/288-dab10db7057b0ce7.js","185","static/chunks/app/layout-46fde1132c555c8e.js"],"Navbar"]
a:I[6112,["699","static/chunks/8e1d74a4-35a4d45ac4ea158f.js","250","static/chunks/250-a256cd67efe5c966.js","374","static/chunks/374-afa9630ac055ee60.js","288","static/chunks/288-dab10db7057b0ce7.js","185","static/chunks/app/layout-46fde1132c555c8e.js"],"Footer"]
3:T4e6,You are a high-level humanoid motion planner and meta-skill developer.

Your task is to decompose a long-horizon natural-language goal into a sequence of
short, atomic subtasks. Each subtask must include a concise "subgoal" (one line of natural language)
and a list of "skills" required to achieve it.

You currently have access to the following SKILL LIBRARY:

{SKILL_LIBRARY}

For every subgoal:
1. Compare the subgoal against the skills in the current library.
2. If ALL required abilities exist, reuse them directly.
3. If the subgoal requires a new ability not in the list, invent a new skill:
   - Use short (1–2 words), descriptive, physically plausible humanoid actions
   - Avoid abstract or cognitive terms
   - Do not use synonyms of existing skills unless functionality differs
   - If no existing skill fully covers the subgoal, you must create a new primitive skill

The final output must be a valid JSON object of the form:

{
  "subtasks": [
    {"subgoal": "<short actionable goal>", "skills": ["Skill1", "Skill2", ...]},
    ...
  ]
}

Guidelines:
- Ensure subtasks are sequential and fully cover the task
- Keep skill names capitalized and concise
- Keep subgoals to one line
- Return only the JSON with no explanations or comments

5:["slug","SkillBlenderLC","d"]
0:["1jGoE5Z9Ej8ONFqULQMX0",[[["",{"children":["projects",{"children":[["slug","SkillBlenderLC","d"],{"children":["__PAGE__?{\"slug\":\"SkillBlenderLC\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["projects",{"children":[["slug","SkillBlenderLC","d"],{"children":["__PAGE__",{},["$L1",["$","article",null,{"className":"container py-10 px-4 md:px-6 max-w-4xl mx-auto","children":[["$","$L2",null,{"href":"/#projects","className":"inline-flex items-center text-sm font-medium text-muted-foreground hover:text-primary mb-8 transition-colors","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left mr-2 h-4 w-4","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],"Back to Projects"]}],["$","div",null,{"className":"prose prose-slate dark:prose-invert max-w-none prose-img:rounded-xl prose-img:shadow-lg prose-headings:scroll-mt-20","children":[["$","h1","h1-0",{"children":"SkillBlenderLC: Self-Evolving Meta-Skill Learning for Humanoid Robots"}],"\n",["$","h2","h2-0",{"children":"Overview"}],"\n",["$","p","p-0",{"children":["SkillBlenderLC is a language-conditioned extension of the ",["$","a","a-0",{"href":"https://github.com/Humanoid-SkillBlender/SkillBlender","children":"SkillBlender"}]," framework that enables autonomous skill discovery and reward learning for humanoid robots. The system leverages Large Language Models (LLMs) to decompose long-horizon tasks, identify missing skills, automatically generate reward functions, and iteratively refine them using reinforcement learning feedback."]}],"\n",["$","p","p-1",{"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/plan.png","alt":"System Overview","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],"\n",["$","hr","hr-0",{}],"\n",["$","h2","h2-1",{"children":"Motivation"}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"Long-horizon humanoid tasks require sequencing and composing multiple motion skills."}],"\n",["$","li","li-1",{"children":"Fixed skill libraries limit scalability and generalization."}],"\n",["$","li","li-2",{"children":"Manual reward design is brittle, time-consuming, and non-scalable."}],"\n",["$","li","li-3",{"children":"Existing LLM-based reward methods do not adapt based on downstream RL performance."}],"\n"]}],"\n",["$","p","p-2",{"children":"SkillBlenderLC addresses these challenges with a self-evolving learning loop that connects language reasoning directly to control learning."}],"\n",["$","hr","hr-1",{}],"\n",["$","h2","h2-2",{"children":"System Design"}],"\n",["$","p","p-3",{"children":"SkillBlenderLC augments the original SkillBlender training stack with three core components:"}],"\n",["$","h3","h3-0",{"children":"1. Task Decomposition"}],"\n",["$","p","p-4",{"children":"An LLM decomposes a natural-language task into ordered subtasks and required skills, proposing new primitive skills when the current library is insufficient."}],"\n",["$","h3","h3-1",{"children":"2. Automatic Reward Bootstrapping"}],"\n",["$","p","p-5",{"children":"For missing skills, the system synthesizes executable Python reward functions using:"}],"\n",["$","ul","ul-1",{"children":["\n",["$","li","li-0",{"children":"Existing reward templates"}],"\n",["$","li","li-1",{"children":"Humanoid environment dynamics"}],"\n"]}],"\n",["$","h3","h3-2",{"children":"3. Reward Refinement via RL Feedback"}],"\n",["$","p","p-6",{"children":"After RL training, performance metrics (stability, convergence, success rate) are fed back to the LLM to refine the reward, enabling iterative improvement and self-evolution."}],"\n",["$","hr","hr-2",{}],"\n",["$","h2","h2-3",{"children":"Experimental Setup"}],"\n",["$","ul","ul-2",{"children":["\n",["$","li","li-0",{"children":[["$","strong","strong-0",{"children":"Simulator"}],": NVIDIA Isaac Gym (PhysX)"]}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Robot"}],": Unitree H1 (19-DoF humanoid)"]}],"\n",["$","li","li-2",{"children":[["$","strong","strong-0",{"children":"RL Algorithm"}],": PPO"]}],"\n",["$","li","li-3",{"children":[["$","strong","strong-0",{"children":"Parallel Environments"}],": 4,096"]}],"\n",["$","li","li-4",{"children":[["$","strong","strong-0",{"children":"Training"}],": 15,000 episodes per skill"]}],"\n"]}],"\n",["$","hr","hr-3",{}],"\n",["$","h2","h2-4",{"children":"LLMs Evaluated"}],"\n",["$","p","p-7",{"children":"We benchmarked six state-of-the-art LLMs across task decomposition and reward generation:"}],"\n",["$","ul","ul-3",{"children":["\n",["$","li","li-0",{"children":"GPT-4o"}],"\n",["$","li","li-1",{"children":"Claude Sonnet 4.5"}],"\n",["$","li","li-2",{"children":"Gemini 2.5 Flash"}],"\n",["$","li","li-3",{"children":"LLaMA 3.3 70B"}],"\n",["$","li","li-4",{"children":"Mistral Large"}],"\n",["$","li","li-5",{"children":"DeepSeek V3"}],"\n"]}],"\n",["$","hr","hr-4",{}],"\n",["$","h2","h2-5",{"children":"Results"}],"\n",["$","h3","h3-3",{"children":"Skill Selection on Long-Horizon Tasks"}],"\n",["$","p","p-8",{"children":"Evaluated on the eight long-horizon tasks from SkillBlender:"}],"\n",["$","p","p-9",{"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/skill_selection_results.png","alt":"Skill Selection Results","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],"\n",["$","p","p-10",{"children":[["$","strong","strong-0",{"children":"Key insight"}],":\nLLMs reliably identify primary motion primitives but often miss secondary or contact-rich skills, motivating downstream reward learning."]}],"\n",["$","hr","hr-5",{}],"\n",["$","h3","h3-4",{"children":"Reward Generation — Primitive Skill: Squatting"}],"\n",["$","p","p-11",{"children":"Performance of policies trained using LLM-generated rewards for the Squatting primitive:"}],"\n",["$","div",null,{"className":"overflow-x-auto my-8","children":["$","table",null,{"children":[["$","thead","thead-0",{"children":["$","tr","tr-0",{"children":[["$","th","th-0",{"style":{"textAlign":"center"},"children":"Ground Truth"}],["$","th","th-1",{"style":{"textAlign":"center"},"children":"GPT-4o"}],["$","th","th-2",{"style":{"textAlign":"center"},"children":"Claude 4.5"}],["$","th","th-3",{"style":{"textAlign":"center"},"children":"Gemini"}]]}]}],["$","tbody","tbody-0",{"children":["$","tr","tr-0",{"children":[["$","td","td-0",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/squat_gt.gif","alt":"GT","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],["$","td","td-1",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/squat_gpt4o.gif","alt":"GPT4o","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],["$","td","td-2",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/squat_sonnet.gif","alt":"Sonnet","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],["$","td","td-3",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/squat_gemini.gif","alt":"Gemini","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}]]}]}]],"className":"w-full border-collapse text-sm"}]}],"\n",["$","div",null,{"className":"overflow-x-auto my-8","children":["$","table",null,{"children":[["$","thead","thead-0",{"children":["$","tr","tr-0",{"children":[["$","th","th-0",{"style":{"textAlign":"center"},"children":"LLaMA"}],["$","th","th-1",{"style":{"textAlign":"center"},"children":"DeepSeek"}],["$","th","th-2",{"style":{"textAlign":"center"},"children":"Mistral"}]]}]}],["$","tbody","tbody-0",{"children":["$","tr","tr-0",{"children":[["$","td","td-0",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/squat_llama.gif","alt":"LLaMA","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],["$","td","td-1",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/squat_deepseek.gif","alt":"DeepSeek","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],["$","td","td-2",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/squat_mistral.gif","alt":"Mistral","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}]]}]}]],"className":"w-full border-collapse text-sm"}]}],"\n",["$","p","p-12",{"children":[["$","strong","strong-0",{"children":"Observation"}],":\nWhile most models reproduce reward structure correctly, numerical misspecification often leads to unstable or ineffective policies."]}],"\n",["$","hr","hr-6",{}],"\n",["$","h3","h3-5",{"children":"Reward Generation — Combined Skill: Button Press"}],"\n",["$","p","p-13",{"children":"The Button Press task requires composing walking and reaching:"}],"\n",["$","div",null,{"className":"overflow-x-auto my-8","children":["$","table",null,{"children":[["$","thead","thead-0",{"children":["$","tr","tr-0",{"children":[["$","th","th-0",{"style":{"textAlign":"center"},"children":"Ground Truth"}],["$","th","th-1",{"style":{"textAlign":"center"},"children":"LLM Generated"}]]}]}],["$","tbody","tbody-0",{"children":["$","tr","tr-0",{"children":[["$","td","td-0",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/buttonpressgt.gif","alt":"GT","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],["$","td","td-1",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/buttonpress.gif","alt":"LLM","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}]]}]}]],"className":"w-full border-collapse text-sm"}]}],"\n",["$","p","p-14",{"children":[["$","strong","strong-0",{"children":"Result"}],":\nOnly Claude-generated rewards successfully enable task completion.\nMost models produce rewards that cause the robot to remain stationary, highlighting the difficulty of multi-stage skill composition."]}],"\n",["$","hr","hr-7",{}],"\n",["$","h3","h3-6",{"children":"Learning New Skills"}],"\n",["$","p","p-15",{"children":"The framework can generate and train rewards for previously unseen skills:"}],"\n",["$","div",null,{"className":"overflow-x-auto my-8","children":["$","table",null,{"children":[["$","thead","thead-0",{"children":["$","tr","tr-0",{"children":[["$","th","th-0",{"style":{"textAlign":"center"},"children":"Bend"}],["$","th","th-1",{"style":{"textAlign":"center"},"children":"Sidestep"}],["$","th","th-2",{"style":{"textAlign":"center"},"children":"Kick"}]]}]}],["$","tbody","tbody-0",{"children":["$","tr","tr-0",{"children":[["$","td","td-0",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/bend_back.gif","alt":"Bend","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],["$","td","td-1",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/sidestep.gif","alt":"Sidestep","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}],["$","td","td-2",{"style":{"textAlign":"center"},"children":["$","span",null,{"className":"block my-8","children":["$","img",null,{"src":"/assets/img/kick.gif","alt":"Kick","className":"rounded-xl shadow-lg mx-auto max-w-full","style":{"maxWidth":"100%","height":"auto","backgroundColor":"transparent","padding":"0"}}]}]}]]}]}]],"className":"w-full border-collapse text-sm"}]}],"\n",["$","p","p-16",{"children":[["$","strong","strong-0",{"children":"Insight"}],":\nSome skills are successfully learned, while others require iterative refinement — validating the need for a closed-loop reward evolution mechanism."]}],"\n",["$","hr","hr-8",{}],"\n",["$","h2","h2-6",{"children":"Prompt Design"}],"\n",["$","p","p-17",{"children":"We design explicit, constrained prompts for each stage to ensure consistency, interpretability, and reproducibility."}],"\n",["$","h3","h3-7",{"children":"Task Decomposition Prompt"}],"\n",["$","pre","pre-0",{"children":["$","code","code-0",{"className":"language-text","children":"$3"}]}],"\n",["$","h3","h3-8",{"children":"Reward Generation Prompt"}],"\n",["$","pre","pre-1",{"children":["$","code","code-0",{"className":"language-text","children":"You are a code-generation agent designing a new humanoid skill reward function.\nReturn only valid Python code defining the reward class.\nDo not include explanations, markdown fences, comments, or introductory text.\n\nBelow are the existing primitive skill rewards currently used:\n<existing_rewards>\n{reward_context}\n</existing_rewards>\n\nAnalyze these to understand parameter conventions and scaling.\n\nBelow is the humanoid environment definition used for training:\n<environment_context>\n{env_context}\n</environment_context>\n\nNow, design a NEW reward class for the following subgoal and skill:\n\nSubgoal: {subgoal}\nSkill: {skill_name}\n\nRules:\n1. Maintain the same Python class structure (class rewards: with nested class scales:)\n2. Reuse ONLY variables, parameters, and scale names already defined\n3. Do NOT invent new variables or physics metrics\n4. Keep numerical scales realistic and consistent with reference rewards\n5. Return only valid Python code, with no explanations or comments\n"}]}],"\n",["$","h3","h3-9",{"children":"Reward Refinement Prompt"}],"\n",["$","pre","pre-2",{"children":["$","code","code-0",{"className":"language-text","children":"You are a humanoid reward designer.\nThe following reward function was trained in RL but performed poorly.\n\nSubgoal: {subgoal}\nSkill: {skill}\n\n[Generated reward]\n{generated_reward}\n\n[Observed RL feedback metrics]\n{feedback_str}\n\nBased on these feedback metrics, rewrite the reward function to:\n1. Improve stability, convergence, and success rate\n2. Keep structure consistent with existing reward format (class rewards: with nested scales)\n3. Keep parameter names consistent\n4. Adjust only scale magnitudes or add missing shaping terms if necessary\n\nOutput only the corrected Python code starting with 'class rewards:'.\nDo NOT include explanations or markdown.\n"}]}]]}]]}],null]]},["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children","$5","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]},["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]},[null,["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":["$","body",null,{"className":"min-h-screen bg-background font-sans antialiased __className_f367f3","children":[["$","$L7",null,{"GA_MEASUREMENT_ID":"G-550NETW0SX"}],["$","$L8",null,{"attribute":"class","forcedTheme":"dark","disableTransitionOnChange":true,"children":["$","div",null,{"className":"relative flex min-h-screen flex-col","children":[["$","$L9",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":null}]}],["$","$La",null,{}]]}]}]]}]}],null]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/2f96b0ef7a1651d4.css","precedence":"next","crossOrigin":""}]],"$Lb"]]]]
b:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Ajay Vikram P - Machine Learning Researcher"}],["$","meta","3",{"name":"description","content":"Portfolio of Ajay Vikram P, a Machine Learning Researcher focused on deep learning and efficient model design."}],["$","link","4",{"rel":"icon","href":"/icon.svg?56c9747835d56d6a","type":"image/svg+xml","sizes":"any"}],["$","meta","5",{"name":"next-size-adjust"}]]
1:null
