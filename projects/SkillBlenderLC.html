<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/assets/img/plan.png"/><link rel="preload" as="image" href="/assets/img/skill_selection_results.png"/><link rel="preload" as="image" href="/assets/img/squat_gt.gif"/><link rel="preload" as="image" href="/assets/img/squat_gpt4o.gif"/><link rel="preload" as="image" href="/assets/img/squat_sonnet.gif"/><link rel="preload" as="image" href="/assets/img/squat_gemini.gif"/><link rel="preload" as="image" href="/assets/img/squat_llama.gif"/><link rel="preload" as="image" href="/assets/img/squat_deepseek.gif"/><link rel="preload" as="image" href="/assets/img/squat_mistral.gif"/><link rel="preload" as="image" href="/assets/img/buttonpressgt.gif"/><link rel="stylesheet" href="/_next/static/css/c746d9bfb1760683.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-4db3efb41e5fd3e4.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-4ff7e63f1a4d62a5.js" async="" crossorigin=""></script><script src="/_next/static/chunks/69-57a7dea2ef833963.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-caaf5cdedf6c1d10.js" async="" crossorigin=""></script><script src="/_next/static/chunks/250-913a093e5b4dc537.js" async=""></script><script src="/_next/static/chunks/app/projects/%5Bslug%5D/page-78f26962105b3138.js" async=""></script><script src="/_next/static/chunks/8e1d74a4-35a4d45ac4ea158f.js" async=""></script><script src="/_next/static/chunks/288-f926f02ea8c25db0.js" async=""></script><script src="/_next/static/chunks/app/layout-fb37c243011b5a6e.js" async=""></script><link rel="preload" as="image" href="/assets/img/buttonpress.gif"/><link rel="preload" as="image" href="/assets/img/bend_back.gif"/><link rel="preload" as="image" href="/assets/img/sidestep.gif"/><link rel="preload" as="image" href="/assets/img/kick.gif"/><title>Ajay Vikram P - Machine Learning Researcher</title><meta name="description" content="Portfolio of Ajay Vikram P, a Machine Learning Researcher focused on deep learning and efficient model design."/><link rel="icon" href="/icon.svg?56c9747835d56d6a" type="image/svg+xml" sizes="any"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="min-h-screen bg-background font-sans antialiased __className_f367f3"><script>!function(){var d=document.documentElement,c=d.classList;c.remove('light','dark');d.style.colorScheme = 'dark';c.add('dark')}()</script><div class="relative flex min-h-screen flex-col"><header class="fixed top-0 w-full z-50 bg-background/80 backdrop-blur-md border-b border-border/40"><div class="container flex h-14 items-center justify-between mx-auto px-4 sm:px-8 max-w-5xl"><a class="font-semibold tracking-tight text-lg hover:opacity-80 transition-opacity" href="/">Ajay Vikram P</a><div class="flex items-center gap-6"><nav class="hidden md:flex gap-6"><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#about">About</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#experience">Experience</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#publications">Publications</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#projects">Projects</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/Resume.pdf">Resume</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#contact">Contact</a></nav></div></div></header><main class="flex-1"><article class="container py-10 px-4 md:px-6 max-w-4xl mx-auto"><a class="inline-flex items-center text-sm font-medium text-muted-foreground hover:text-primary mb-8 transition-colors" href="/#projects"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left mr-2 h-4 w-4"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg>Back to Projects</a><div class="prose prose-slate dark:prose-invert max-w-none prose-img:rounded-xl prose-img:shadow-lg prose-headings:scroll-mt-20"><h1>SkillBlenderLC: Self-Evolving Meta-Skill Learning for Humanoid Robots</h1>
<h2>Overview</h2>
<p>SkillBlenderLC is a language-conditioned extension of the <a href="https://github.com/Humanoid-SkillBlender/SkillBlender">SkillBlender</a> framework that enables autonomous skill discovery and reward learning for humanoid robots. The system leverages Large Language Models (LLMs) to decompose long-horizon tasks, identify missing skills, automatically generate reward functions, and iteratively refine them using reinforcement learning feedback.</p>
<p><span class="block my-8"><img src="/assets/img/plan.png" alt="System Overview" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></p>
<hr/>
<h2>Motivation</h2>
<ul>
<li>Long-horizon humanoid tasks require sequencing and composing multiple motion skills.</li>
<li>Fixed skill libraries limit scalability and generalization.</li>
<li>Manual reward design is brittle, time-consuming, and non-scalable.</li>
<li>Existing LLM-based reward methods do not adapt based on downstream RL performance.</li>
</ul>
<p>SkillBlenderLC addresses these challenges with a self-evolving learning loop that connects language reasoning directly to control learning.</p>
<hr/>
<h2>System Design</h2>
<p>SkillBlenderLC augments the original SkillBlender training stack with three core components:</p>
<h3>1. Task Decomposition</h3>
<p>An LLM decomposes a natural-language task into ordered subtasks and required skills, proposing new primitive skills when the current library is insufficient.</p>
<h3>2. Automatic Reward Bootstrapping</h3>
<p>For missing skills, the system synthesizes executable Python reward functions using:</p>
<ul>
<li>Existing reward templates</li>
<li>Humanoid environment dynamics</li>
</ul>
<h3>3. Reward Refinement via RL Feedback</h3>
<p>After RL training, performance metrics (stability, convergence, success rate) are fed back to the LLM to refine the reward, enabling iterative improvement and self-evolution.</p>
<hr/>
<h2>Experimental Setup</h2>
<ul>
<li><strong>Simulator</strong>: NVIDIA Isaac Gym (PhysX)</li>
<li><strong>Robot</strong>: Unitree H1 (19-DoF humanoid)</li>
<li><strong>RL Algorithm</strong>: PPO</li>
<li><strong>Parallel Environments</strong>: 4,096</li>
<li><strong>Training</strong>: 15,000 episodes per skill</li>
</ul>
<hr/>
<h2>LLMs Evaluated</h2>
<p>We benchmarked six state-of-the-art LLMs across task decomposition and reward generation:</p>
<ul>
<li>GPT-4o</li>
<li>Claude Sonnet 4.5</li>
<li>Gemini 2.5 Flash</li>
<li>LLaMA 3.3 70B</li>
<li>Mistral Large</li>
<li>DeepSeek V3</li>
</ul>
<hr/>
<h2>Results</h2>
<h3>Skill Selection on Long-Horizon Tasks</h3>
<p>Evaluated on the eight long-horizon tasks from SkillBlender:</p>
<p><span class="block my-8"><img src="/assets/img/skill_selection_results.png" alt="Skill Selection Results" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></p>
<p><strong>Key insight</strong>:
LLMs reliably identify primary motion primitives but often miss secondary or contact-rich skills, motivating downstream reward learning.</p>
<hr/>
<h3>Reward Generation — Primitive Skill: Squatting</h3>
<p>Performance of policies trained using LLM-generated rewards for the Squatting primitive:</p>
<div class="overflow-x-auto my-8"><table class="w-full border-collapse text-sm"><thead><tr><th style="text-align:center">Ground Truth</th><th style="text-align:center">GPT-4o</th><th style="text-align:center">Claude 4.5</th><th style="text-align:center">Gemini</th></tr></thead><tbody><tr><td style="text-align:center"><span class="block my-8"><img src="/assets/img/squat_gt.gif" alt="GT" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/squat_gpt4o.gif" alt="GPT4o" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/squat_sonnet.gif" alt="Sonnet" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/squat_gemini.gif" alt="Gemini" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td></tr></tbody></table></div>
<div class="overflow-x-auto my-8"><table class="w-full border-collapse text-sm"><thead><tr><th style="text-align:center">LLaMA</th><th style="text-align:center">DeepSeek</th><th style="text-align:center">Mistral</th></tr></thead><tbody><tr><td style="text-align:center"><span class="block my-8"><img src="/assets/img/squat_llama.gif" alt="LLaMA" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/squat_deepseek.gif" alt="DeepSeek" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/squat_mistral.gif" alt="Mistral" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td></tr></tbody></table></div>
<p><strong>Observation</strong>:
While most models reproduce reward structure correctly, numerical misspecification often leads to unstable or ineffective policies.</p>
<hr/>
<h3>Reward Generation — Combined Skill: Button Press</h3>
<p>The Button Press task requires composing walking and reaching:</p>
<div class="overflow-x-auto my-8"><table class="w-full border-collapse text-sm"><thead><tr><th style="text-align:center">Ground Truth</th><th style="text-align:center">LLM Generated</th></tr></thead><tbody><tr><td style="text-align:center"><span class="block my-8"><img src="/assets/img/buttonpressgt.gif" alt="GT" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/buttonpress.gif" alt="LLM" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td></tr></tbody></table></div>
<p><strong>Result</strong>:
Only Claude-generated rewards successfully enable task completion.
Most models produce rewards that cause the robot to remain stationary, highlighting the difficulty of multi-stage skill composition.</p>
<hr/>
<h3>Learning New Skills</h3>
<p>The framework can generate and train rewards for previously unseen skills:</p>
<div class="overflow-x-auto my-8"><table class="w-full border-collapse text-sm"><thead><tr><th style="text-align:center">Bend</th><th style="text-align:center">Sidestep</th><th style="text-align:center">Kick</th></tr></thead><tbody><tr><td style="text-align:center"><span class="block my-8"><img src="/assets/img/bend_back.gif" alt="Bend" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/sidestep.gif" alt="Sidestep" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/kick.gif" alt="Kick" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td></tr></tbody></table></div>
<p><strong>Insight</strong>:
Some skills are successfully learned, while others require iterative refinement — validating the need for a closed-loop reward evolution mechanism.</p>
<hr/>
<h2>Prompt Design</h2>
<p>We design explicit, constrained prompts for each stage to ensure consistency, interpretability, and reproducibility.</p>
<h3>Task Decomposition Prompt</h3>
<pre><code class="language-text">You are a high-level humanoid motion planner and meta-skill developer.

Your task is to decompose a long-horizon natural-language goal into a sequence of
short, atomic subtasks. Each subtask must include a concise &quot;subgoal&quot; (one line of natural language)
and a list of &quot;skills&quot; required to achieve it.

You currently have access to the following SKILL LIBRARY:

{SKILL_LIBRARY}

For every subgoal:
1. Compare the subgoal against the skills in the current library.
2. If ALL required abilities exist, reuse them directly.
3. If the subgoal requires a new ability not in the list, invent a new skill:
   - Use short (1–2 words), descriptive, physically plausible humanoid actions
   - Avoid abstract or cognitive terms
   - Do not use synonyms of existing skills unless functionality differs
   - If no existing skill fully covers the subgoal, you must create a new primitive skill

The final output must be a valid JSON object of the form:

{
  &quot;subtasks&quot;: [
    {&quot;subgoal&quot;: &quot;&lt;short actionable goal&gt;&quot;, &quot;skills&quot;: [&quot;Skill1&quot;, &quot;Skill2&quot;, ...]},
    ...
  ]
}

Guidelines:
- Ensure subtasks are sequential and fully cover the task
- Keep skill names capitalized and concise
- Keep subgoals to one line
- Return only the JSON with no explanations or comments

</code></pre>
<h3>Reward Generation Prompt</h3>
<pre><code class="language-text">You are a code-generation agent designing a new humanoid skill reward function.
Return only valid Python code defining the reward class.
Do not include explanations, markdown fences, comments, or introductory text.

Below are the existing primitive skill rewards currently used:
&lt;existing_rewards&gt;
{reward_context}
&lt;/existing_rewards&gt;

Analyze these to understand parameter conventions and scaling.

Below is the humanoid environment definition used for training:
&lt;environment_context&gt;
{env_context}
&lt;/environment_context&gt;

Now, design a NEW reward class for the following subgoal and skill:

Subgoal: {subgoal}
Skill: {skill_name}

Rules:
1. Maintain the same Python class structure (class rewards: with nested class scales:)
2. Reuse ONLY variables, parameters, and scale names already defined
3. Do NOT invent new variables or physics metrics
4. Keep numerical scales realistic and consistent with reference rewards
5. Return only valid Python code, with no explanations or comments
</code></pre>
<h3>Reward Refinement Prompt</h3>
<pre><code class="language-text">You are a humanoid reward designer.
The following reward function was trained in RL but performed poorly.

Subgoal: {subgoal}
Skill: {skill}

[Generated reward]
{generated_reward}

[Observed RL feedback metrics]
{feedback_str}

Based on these feedback metrics, rewrite the reward function to:
1. Improve stability, convergence, and success rate
2. Keep structure consistent with existing reward format (class rewards: with nested scales)
3. Keep parameter names consistent
4. Adjust only scale magnitudes or add missing shaping terms if necessary

Output only the corrected Python code starting with &#x27;class rewards:&#x27;.
Do NOT include explanations or markdown.
</code></pre></div></article></main><footer class="py-6 border-t border-border/50"><div class="container flex flex-col items-center justify-between gap-4 md:flex-row mx-auto max-w-5xl px-4 md:px-6"><p class="text-[10px] text-muted-foreground">© <!-- -->2026<!-- --> <!-- -->Ajay Vikram P</p><div class="flex gap-4 items-center"><a href="https://github.com/ajay-vikram" target="_blank" rel="noopener noreferrer" class="text-muted-foreground hover:text-foreground transition-colors" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="w-3.5 h-3.5" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://www.linkedin.com/in/ajay-vikram777/" target="_blank" rel="noopener noreferrer" class="text-muted-foreground hover:text-foreground transition-colors" aria-label="LinkedIn"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="w-3.5 h-3.5" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://medium.com/@ajayvikramp" target="_blank" rel="noopener noreferrer" class="text-muted-foreground hover:text-foreground transition-colors" aria-label="Medium"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="w-3.5 h-3.5" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 32v448h448V32H0zm372.2 106.1l-24 23c-2.1 1.6-3.1 4.2-2.7 6.7v169.3c-.4 2.6.6 5.2 2.7 6.7l23.5 23v5.1h-118V367l24.3-23.6c2.4-2.4 2.4-3.1 2.4-6.7V199.8l-67.6 171.6h-9.1L125 199.8v115c-.7 4.8 1 9.7 4.4 13.2l31.6 38.3v5.1H71.2v-5.1l31.6-38.3c3.4-3.5 4.9-8.4 4.1-13.2v-133c.4-3.7-1-7.3-3.8-9.8L75 138.1V133h87.3l67.4 148L289 133.1h83.2v5z"></path></svg></a></div></div></footer></div><script src="/_next/static/chunks/webpack-4db3efb41e5fd3e4.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/c746d9bfb1760683.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:I[7690,[],\"\"]\n6:I[5250,[\"250\",\"static/chunks/250-913a093e5b4dc537.js\",\"101\",\"static/chunks/app/projects/%5Bslug%5D/page-78f26962105b3138.js\"],\"\"]\n8:I[5613,[],\"\"]\na:I[1778,[],\"\"]\nb:I[9968,[\"699\",\"static/chunks/8e1d74a4-35a4d45ac4ea158f.js\",\"250\",\"static/chunks/250-913a093e5b4dc537.js\",\"288\",\"static/chunks/288-f926f02ea8c25db0.js\",\"185\",\"static/chunks/app/layout-fb37c243011b5a6e.js\"],\"ThemeProvider\"]\nc:I[4002,[\"699\",\"static/chunks/8e1d74a4-35a4d45ac4ea158f.js\",\"250\",\"static/chunks/250-913a093e5b4dc537.js\",\""])</script><script>self.__next_f.push([1,"288\",\"static/chunks/288-f926f02ea8c25db0.js\",\"185\",\"static/chunks/app/layout-fb37c243011b5a6e.js\"],\"Navbar\"]\nd:I[6112,[\"699\",\"static/chunks/8e1d74a4-35a4d45ac4ea158f.js\",\"250\",\"static/chunks/250-913a093e5b4dc537.js\",\"288\",\"static/chunks/288-f926f02ea8c25db0.js\",\"185\",\"static/chunks/app/layout-fb37c243011b5a6e.js\"],\"Footer\"]\nf:I[8955,[],\"\"]\n7:T4e6,"])</script><script>self.__next_f.push([1,"You are a high-level humanoid motion planner and meta-skill developer.\n\nYour task is to decompose a long-horizon natural-language goal into a sequence of\nshort, atomic subtasks. Each subtask must include a concise \"subgoal\" (one line of natural language)\nand a list of \"skills\" required to achieve it.\n\nYou currently have access to the following SKILL LIBRARY:\n\n{SKILL_LIBRARY}\n\nFor every subgoal:\n1. Compare the subgoal against the skills in the current library.\n2. If ALL required abilities exist, reuse them directly.\n3. If the subgoal requires a new ability not in the list, invent a new skill:\n   - Use short (1–2 words), descriptive, physically plausible humanoid actions\n   - Avoid abstract or cognitive terms\n   - Do not use synonyms of existing skills unless functionality differs\n   - If no existing skill fully covers the subgoal, you must create a new primitive skill\n\nThe final output must be a valid JSON object of the form:\n\n{\n  \"subtasks\": [\n    {\"subgoal\": \"\u003cshort actionable goal\u003e\", \"skills\": [\"Skill1\", \"Skill2\", ...]},\n    ...\n  ]\n}\n\nGuidelines:\n- Ensure subtasks are sequential and fully cover the task\n- Keep skill names capitalized and concise\n- Keep subgoals to one line\n- Return only the JSON with no explanations or comments\n\n"])</script><script>self.__next_f.push([1,"9:[\"slug\",\"SkillBlenderLC\",\"d\"]\n10:[]\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c746d9bfb1760683.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"fWB6xaBSjVzhTYJE1Zs6P\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/projects/SkillBlenderLC\",\"initialTree\":[\"\",{\"children\":[\"projects\",{\"children\":[[\"slug\",\"SkillBlenderLC\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"SkillBlenderLC\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"projects\",{\"children\":[[\"slug\",\"SkillBlenderLC\",\"d\"],{\"children\":[\"__PAGE__\",{},[\"$L5\",[\"$\",\"article\",null,{\"className\":\"container py-10 px-4 md:px-6 max-w-4xl mx-auto\",\"children\":[[\"$\",\"$L6\",null,{\"href\":\"/#projects\",\"className\":\"inline-flex items-center text-sm font-medium text-muted-foreground hover:text-primary mb-8 transition-colors\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-left mr-2 h-4 w-4\",\"children\":[[\"$\",\"path\",\"1l729n\",{\"d\":\"m12 19-7-7 7-7\"}],[\"$\",\"path\",\"x3x0zl\",{\"d\":\"M19 12H5\"}],\"$undefined\"]}],\"Back to Projects\"]}],[\"$\",\"div\",null,{\"className\":\"prose prose-slate dark:prose-invert max-w-none prose-img:rounded-xl prose-img:shadow-lg prose-headings:scroll-mt-20\",\"children\":[[\"$\",\"h1\",\"h1-0\",{\"children\":\"SkillBlenderLC: Self-Evolving Meta-Skill Learning for Humanoid Robots\"}],\"\\n\",[\"$\",\"h2\",\"h2-0\",{\"children\":\"Overview\"}],\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[\"SkillBlenderLC is a language-conditioned extension of the \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://github.com/Humanoid-SkillBlender/SkillBlender\",\"children\":\"SkillBlender\"}],\" framework that enables autonomous skill discovery and reward learning for humanoid robots. The system leverages Large Language Models (LLMs) to decompose long-horizon tasks, identify missing skills, automatically generate reward functions, and iteratively refine them using reinforcement learning feedback.\"]}],\"\\n\",[\"$\",\"p\",\"p-1\",{\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/plan.png\",\"alt\":\"System Overview\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],\"\\n\",[\"$\",\"hr\",\"hr-0\",{}],\"\\n\",[\"$\",\"h2\",\"h2-1\",{\"children\":\"Motivation\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Long-horizon humanoid tasks require sequencing and composing multiple motion skills.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Fixed skill libraries limit scalability and generalization.\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Manual reward design is brittle, time-consuming, and non-scalable.\"}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":\"Existing LLM-based reward methods do not adapt based on downstream RL performance.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"p-2\",{\"children\":\"SkillBlenderLC addresses these challenges with a self-evolving learning loop that connects language reasoning directly to control learning.\"}],\"\\n\",[\"$\",\"hr\",\"hr-1\",{}],\"\\n\",[\"$\",\"h2\",\"h2-2\",{\"children\":\"System Design\"}],\"\\n\",[\"$\",\"p\",\"p-3\",{\"children\":\"SkillBlenderLC augments the original SkillBlender training stack with three core components:\"}],\"\\n\",[\"$\",\"h3\",\"h3-0\",{\"children\":\"1. Task Decomposition\"}],\"\\n\",[\"$\",\"p\",\"p-4\",{\"children\":\"An LLM decomposes a natural-language task into ordered subtasks and required skills, proposing new primitive skills when the current library is insufficient.\"}],\"\\n\",[\"$\",\"h3\",\"h3-1\",{\"children\":\"2. Automatic Reward Bootstrapping\"}],\"\\n\",[\"$\",\"p\",\"p-5\",{\"children\":\"For missing skills, the system synthesizes executable Python reward functions using:\"}],\"\\n\",[\"$\",\"ul\",\"ul-1\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Existing reward templates\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Humanoid environment dynamics\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",\"h3-2\",{\"children\":\"3. Reward Refinement via RL Feedback\"}],\"\\n\",[\"$\",\"p\",\"p-6\",{\"children\":\"After RL training, performance metrics (stability, convergence, success rate) are fed back to the LLM to refine the reward, enabling iterative improvement and self-evolution.\"}],\"\\n\",[\"$\",\"hr\",\"hr-2\",{}],\"\\n\",[\"$\",\"h2\",\"h2-3\",{\"children\":\"Experimental Setup\"}],\"\\n\",[\"$\",\"ul\",\"ul-2\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Simulator\"}],\": NVIDIA Isaac Gym (PhysX)\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Robot\"}],\": Unitree H1 (19-DoF humanoid)\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"RL Algorithm\"}],\": PPO\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Parallel Environments\"}],\": 4,096\"]}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Training\"}],\": 15,000 episodes per skill\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"hr\",\"hr-3\",{}],\"\\n\",[\"$\",\"h2\",\"h2-4\",{\"children\":\"LLMs Evaluated\"}],\"\\n\",[\"$\",\"p\",\"p-7\",{\"children\":\"We benchmarked six state-of-the-art LLMs across task decomposition and reward generation:\"}],\"\\n\",[\"$\",\"ul\",\"ul-3\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"GPT-4o\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Claude Sonnet 4.5\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Gemini 2.5 Flash\"}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":\"LLaMA 3.3 70B\"}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":\"Mistral Large\"}],\"\\n\",[\"$\",\"li\",\"li-5\",{\"children\":\"DeepSeek V3\"}],\"\\n\"]}],\"\\n\",[\"$\",\"hr\",\"hr-4\",{}],\"\\n\",[\"$\",\"h2\",\"h2-5\",{\"children\":\"Results\"}],\"\\n\",[\"$\",\"h3\",\"h3-3\",{\"children\":\"Skill Selection on Long-Horizon Tasks\"}],\"\\n\",[\"$\",\"p\",\"p-8\",{\"children\":\"Evaluated on the eight long-horizon tasks from SkillBlender:\"}],\"\\n\",[\"$\",\"p\",\"p-9\",{\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/skill_selection_results.png\",\"alt\":\"Skill Selection Results\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],\"\\n\",[\"$\",\"p\",\"p-10\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Key insight\"}],\":\\nLLMs reliably identify primary motion primitives but often miss secondary or contact-rich skills, motivating downstream reward learning.\"]}],\"\\n\",[\"$\",\"hr\",\"hr-5\",{}],\"\\n\",[\"$\",\"h3\",\"h3-4\",{\"children\":\"Reward Generation — Primitive Skill: Squatting\"}],\"\\n\",[\"$\",\"p\",\"p-11\",{\"children\":\"Performance of policies trained using LLM-generated rewards for the Squatting primitive:\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"overflow-x-auto my-8\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",\"thead-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"th\",\"th-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Ground Truth\"}],[\"$\",\"th\",\"th-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"GPT-4o\"}],[\"$\",\"th\",\"th-2\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Claude 4.5\"}],[\"$\",\"th\",\"th-3\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Gemini\"}]]}]}],[\"$\",\"tbody\",\"tbody-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/squat_gt.gif\",\"alt\":\"GT\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/squat_gpt4o.gif\",\"alt\":\"GPT4o\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/squat_sonnet.gif\",\"alt\":\"Sonnet\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-3\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/squat_gemini.gif\",\"alt\":\"Gemini\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}]]}]}]],\"className\":\"w-full border-collapse text-sm\"}]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"overflow-x-auto my-8\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",\"thead-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"th\",\"th-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"LLaMA\"}],[\"$\",\"th\",\"th-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"DeepSeek\"}],[\"$\",\"th\",\"th-2\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Mistral\"}]]}]}],[\"$\",\"tbody\",\"tbody-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/squat_llama.gif\",\"alt\":\"LLaMA\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/squat_deepseek.gif\",\"alt\":\"DeepSeek\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/squat_mistral.gif\",\"alt\":\"Mistral\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}]]}]}]],\"className\":\"w-full border-collapse text-sm\"}]}],\"\\n\",[\"$\",\"p\",\"p-12\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Observation\"}],\":\\nWhile most models reproduce reward structure correctly, numerical misspecification often leads to unstable or ineffective policies.\"]}],\"\\n\",[\"$\",\"hr\",\"hr-6\",{}],\"\\n\",[\"$\",\"h3\",\"h3-5\",{\"children\":\"Reward Generation — Combined Skill: Button Press\"}],\"\\n\",[\"$\",\"p\",\"p-13\",{\"children\":\"The Button Press task requires composing walking and reaching:\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"overflow-x-auto my-8\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",\"thead-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"th\",\"th-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Ground Truth\"}],[\"$\",\"th\",\"th-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"LLM Generated\"}]]}]}],[\"$\",\"tbody\",\"tbody-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/buttonpressgt.gif\",\"alt\":\"GT\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/buttonpress.gif\",\"alt\":\"LLM\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}]]}]}]],\"className\":\"w-full border-collapse text-sm\"}]}],\"\\n\",[\"$\",\"p\",\"p-14\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Result\"}],\":\\nOnly Claude-generated rewards successfully enable task completion.\\nMost models produce rewards that cause the robot to remain stationary, highlighting the difficulty of multi-stage skill composition.\"]}],\"\\n\",[\"$\",\"hr\",\"hr-7\",{}],\"\\n\",[\"$\",\"h3\",\"h3-6\",{\"children\":\"Learning New Skills\"}],\"\\n\",[\"$\",\"p\",\"p-15\",{\"children\":\"The framework can generate and train rewards for previously unseen skills:\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"overflow-x-auto my-8\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",\"thead-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"th\",\"th-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Bend\"}],[\"$\",\"th\",\"th-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Sidestep\"}],[\"$\",\"th\",\"th-2\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Kick\"}]]}]}],[\"$\",\"tbody\",\"tbody-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/bend_back.gif\",\"alt\":\"Bend\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/sidestep.gif\",\"alt\":\"Sidestep\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/kick.gif\",\"alt\":\"Kick\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}]]}]}]],\"className\":\"w-full border-collapse text-sm\"}]}],\"\\n\",[\"$\",\"p\",\"p-16\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Insight\"}],\":\\nSome skills are successfully learned, while others require iterative refinement — validating the need for a closed-loop reward evolution mechanism.\"]}],\"\\n\",[\"$\",\"hr\",\"hr-8\",{}],\"\\n\",[\"$\",\"h2\",\"h2-6\",{\"children\":\"Prompt Design\"}],\"\\n\",[\"$\",\"p\",\"p-17\",{\"children\":\"We design explicit, constrained prompts for each stage to ensure consistency, interpretability, and reproducibility.\"}],\"\\n\",[\"$\",\"h3\",\"h3-7\",{\"children\":\"Task Decomposition Prompt\"}],\"\\n\",[\"$\",\"pre\",\"pre-0\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"language-text\",\"children\":\"$7\"}]}],\"\\n\",[\"$\",\"h3\",\"h3-8\",{\"children\":\"Reward Generation Prompt\"}],\"\\n\",[\"$\",\"pre\",\"pre-1\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"language-text\",\"children\":\"You are a code-generation agent designing a new humanoid skill reward function.\\nReturn only valid Python code defining the reward class.\\nDo not include explanations, markdown fences, comments, or introductory text.\\n\\nBelow are the existing primitive skill rewards currently used:\\n\u003cexisting_rewards\u003e\\n{reward_context}\\n\u003c/existing_rewards\u003e\\n\\nAnalyze these to understand parameter conventions and scaling.\\n\\nBelow is the humanoid environment definition used for training:\\n\u003cenvironment_context\u003e\\n{env_context}\\n\u003c/environment_context\u003e\\n\\nNow, design a NEW reward class for the following subgoal and skill:\\n\\nSubgoal: {subgoal}\\nSkill: {skill_name}\\n\\nRules:\\n1. Maintain the same Python class structure (class rewards: with nested class scales:)\\n2. Reuse ONLY variables, parameters, and scale names already defined\\n3. Do NOT invent new variables or physics metrics\\n4. Keep numerical scales realistic and consistent with reference rewards\\n5. Return only valid Python code, with no explanations or comments\\n\"}]}],\"\\n\",[\"$\",\"h3\",\"h3-9\",{\"children\":\"Reward Refinement Prompt\"}],\"\\n\",[\"$\",\"pre\",\"pre-2\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"language-text\",\"children\":\"You are a humanoid reward designer.\\nThe following reward function was trained in RL but performed poorly.\\n\\nSubgoal: {subgoal}\\nSkill: {skill}\\n\\n[Generated reward]\\n{generated_reward}\\n\\n[Observed RL feedback metrics]\\n{feedback_str}\\n\\nBased on these feedback metrics, rewrite the reward function to:\\n1. Improve stability, convergence, and success rate\\n2. Keep structure consistent with existing reward format (class rewards: with nested scales)\\n3. Keep parameter names consistent\\n4. Adjust only scale magnitudes or add missing shaping terms if necessary\\n\\nOutput only the corrected Python code starting with 'class rewards:'.\\nDo NOT include explanations or markdown.\\n\"}]}]]}]]}],null]]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"projects\",\"children\",\"$9\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"projects\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-screen bg-background font-sans antialiased __className_f367f3\",\"children\":[\"$\",\"$Lb\",null,{\"attribute\":\"class\",\"forcedTheme\":\"dark\",\"disableTransitionOnChange\":true,\"children\":[\"$\",\"div\",null,{\"className\":\"relative flex min-h-screen flex-col\",\"children\":[[\"$\",\"$Lc\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"$Ld\",null,{}]]}]}]}]}],null]],\"initialHead\":[false,\"$Le\"],\"globalErrorComponent\":\"$f\",\"missingSlots\":\"$W10\"}]]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Ajay Vikram P - Machine Learning Researcher\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Portfolio of Ajay Vikram P, a Machine Learning Researcher focused on deep learning and efficient model design.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/icon.svg?56c9747835d56d6a\",\"type\":\"image/svg+xml\",\"sizes\":\"any\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"5:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>