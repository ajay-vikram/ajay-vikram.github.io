<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/assets/img/spectrograms.png"/><link rel="preload" as="image" href="/assets/img/CNN.png"/><link rel="preload" as="image" href="/assets/img/fp32weights.png"/><link rel="preload" as="image" href="/assets/img/ternaryweights.png"/><link rel="preload" as="image" href="/assets/img/QAT.png"/><link rel="preload" as="image" href="/assets/img/int_inference.png"/><link rel="stylesheet" href="/_next/static/css/7aacf04632d0f10b.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-e696bf7940c5214b.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-4ff7e63f1a4d62a5.js" async="" crossorigin=""></script><script src="/_next/static/chunks/69-57a7dea2ef833963.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-caaf5cdedf6c1d10.js" async="" crossorigin=""></script><script src="/_next/static/chunks/250-913a093e5b4dc537.js" async=""></script><script src="/_next/static/chunks/app/projects/%5Bslug%5D/page-78f26962105b3138.js" async=""></script><script src="/_next/static/chunks/8e1d74a4-35a4d45ac4ea158f.js" async=""></script><script src="/_next/static/chunks/288-dab10db7057b0ce7.js" async=""></script><script src="/_next/static/chunks/app/layout-2518eb680be61c66.js" async=""></script><title>Ajay Vikram P - Machine Learning Researcher</title><meta name="description" content="Portfolio of Ajay Vikram P, a Machine Learning Researcher focused on deep learning and efficient model design."/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="min-h-screen bg-background font-sans antialiased __className_f367f3"><script>!function(){var d=document.documentElement,c=d.classList;c.remove('light','dark');d.style.colorScheme = 'dark';c.add('dark')}()</script><div class="relative flex min-h-screen flex-col"><header class="fixed top-0 w-full z-50 bg-background/80 backdrop-blur-md border-b border-border/40"><div class="container flex h-14 items-center justify-between mx-auto px-4 sm:px-8 max-w-5xl"><a class="font-semibold tracking-tight text-lg hover:opacity-80 transition-opacity" href="/">Ajay Vikram P</a><div class="flex items-center gap-6"><nav class="hidden md:flex gap-6"><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#about">About</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#experience">Experience</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#publications">Publications</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#projects">Projects</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/Resume.pdf">Resume</a><a class="text-sm font-medium text-muted-foreground transition-colors hover:text-foreground" href="/#contact">Contact</a></nav></div></div></header><main class="flex-1"><article class="container py-10 px-4 md:px-6 max-w-4xl mx-auto"><a class="inline-flex items-center text-sm font-medium text-muted-foreground hover:text-primary mb-8 transition-colors" href="/#projects"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left mr-2 h-4 w-4"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg>Back to Projects</a><div class="prose prose-slate dark:prose-invert max-w-none prose-img:rounded-xl prose-img:shadow-lg prose-headings:scroll-mt-20"><h1>Ternarizing CNNs for Human Activity Recognition</h1>
<h2>Overview</h2>
<p>This project focuses on compressing a CNN model for human activity recognition using ternarization and quantization techniques. The aim is to significantly reduce model size and computational complexity without sacrificing much accuracy. The baseline model, trained on 64×64 spectrograms derived from radar signals, is ternarized and quantized to allow deployment with integer-only inference on resource-constrained embedded platforms.</p>
<h2>Dataset</h2>
<p>The model is trained on a private dataset, using spectrogram representations of radar signals, each sized <code>64×64×1</code>. These spectrograms capture temporal-frequency features of radar reflections and are well-suited for classifying fine-grained human activities. The dataset includes 5 distinct activity classes - <code>{jogging, jumping, situp, waving, other}</code>, and training was performed using 5-fold cross-validation to ensure robust generalization. The spectrograms look like:</p>
<p><span class="block my-8"><img src="/assets/img/spectrograms.png" alt="Input Spectrograms" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></p>
<h2>Model Architecture</h2>
<p>The CNN architecture is lightweight, consisting of:</p>
<ul>
<li>A 2D convolution layer</li>
<li>Two depthwise separable convolutions for spatial-temporal filtering</li>
<li>Fully connected output layer with 5 class logits</li>
</ul>
<p><span class="block my-8"><img src="/assets/img/CNN.png" alt="CNN" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:white;padding:10px"/></span></p>
<p>The model had 3029 parameters in total and was designed with efficiency in mind, enabling rapid inference while maintaining high classification accuracy.</p>
<h2>Ternarization &amp; Quantization</h2>
<p>Quantization is the process of reducing the precision of numbers in a model—such as weights, biases and activations—by representing them with fewer bits. This helps make machine learning models smaller, faster, and more efficient for deployment on devices with limited resources. Ternarization uses 2 bits to represent each weight, restricting values to three discrete levels.</p>
<p>The weight ternarization step maps full-precision weights to one of three values: {−1, 0, +1}. To ensure the ternary weight networks perform well, it is required to minimize the Euclidian distance between the full precision weights W and the ternary-valued weights W&#x27; while including a non-negative scaling factor α. Ternarization reduces the model size by over 16× and enables efficient bitwise operations. A threshold-based approach was used to determine which weights are quantized to -1, 0 and 1. In addition, the activations and biases were quantized to 8-bit and 23-bit respectively. Quantization was done using uniform affine mapping with a learned scale and zero-point, allowing real values to be approximated as <code>q = round(r/scale) + zero_point</code>. The images below illustrate the floating-point weights and biases on the left, and their ternarized and quantized counterparts on the right.</p>
<div class="overflow-x-auto my-8"><table class="w-full border-collapse text-sm"><thead><tr><th style="text-align:center">Floating Point Weights</th><th style="text-align:center">Ternarized Integer Weights</th></tr></thead><tbody><tr><td style="text-align:center"><span class="block my-8"><img src="/assets/img/fp32weights.png" alt="FP32" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td><td style="text-align:center"><span class="block my-8"><img src="/assets/img/ternaryweights.png" alt="Ternary" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></td></tr></tbody></table></div>
<p>The model was trained using Quantization-Aware Training (QAT). Unlike post-training quantization, QAT simulates quantization effects during training itself, enabling the network to learn robust representations under low-bit constraints. During training, fake quantization modules simulate low-bit precision, while the actual parameters remain in float for gradient updates. The Straight-Through Estimator (STE) was used to approximate gradients through the non-differentiable <code>torch.round</code> quantization function.</p>
<p><span class="block my-8"><img src="/assets/img/QAT.png" alt="QAT" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></p>
<p>To optimize further for inference, pruning was done to achieve ~45% sparsity and batch normalization layers were folded into the preceding convolutional layers, combining their parameters with the conv weights and biases. This reduced runtime complexity without changing model behavior.</p>
<p>Finally, all operations—including convolutions, batch norm, activation, and pooling—were implemented using integer-only arithmetic. Dyadic scaling factors of the form (s = a ⁄ 2ᵇ), where a and b are integers, were used for efficient scaling via bit-shifts instead of division, allowing deployment on fixed-point hardware with no floating-point support. This full quantization pipeline enabled robust, low-latency inference with minimal accuracy degradation compared to the full-precision baseline.</p>
<p><span class="block my-8"><img src="/assets/img/int_inference.png" alt="Integer-Only Inference" class="rounded-xl shadow-lg mx-auto max-w-full" style="max-width:100%;height:auto;background-color:transparent;padding:0"/></span></p>
<h2>Results</h2>
<p>The model was evaluated using 5-fold cross-validation. Results below demonstrate only a marginal drop in accuracy post-ternarization:</p>
<div class="overflow-x-auto my-8"><table class="w-full border-collapse text-sm"><thead><tr><th style="text-align:left">Fold</th><th style="text-align:left">FP32 Accuracy (%)</th><th style="text-align:left">Ternarized Accuracy (%)</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:left">99.27</td><td style="text-align:left">98.19</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left">100.00</td><td style="text-align:left">99.64</td></tr><tr><td style="text-align:left">3</td><td style="text-align:left">99.27</td><td style="text-align:left">98.91</td></tr><tr><td style="text-align:left">4</td><td style="text-align:left">98.55</td><td style="text-align:left">97.83</td></tr><tr><td style="text-align:left">5</td><td style="text-align:left">100.00</td><td style="text-align:left">100.00</td></tr><tr><td style="text-align:left"><strong>Avg</strong></td><td style="text-align:left"><strong>99.42</strong></td><td style="text-align:left"><strong>98.91</strong></td></tr></tbody></table></div>
<p>Despite ~45% sparsity and ternarized weights, the accuracy remained close to baseline, validating the effectiveness of ternarization and quantization for embedded HAR systems.</p></div></article></main><footer class="py-6 border-t border-border/50"><div class="container flex flex-col items-center justify-between gap-4 md:flex-row mx-auto max-w-5xl px-4 md:px-6"><p class="text-[10px] text-muted-foreground">© <!-- -->2026<!-- --> <!-- -->Ajay Vikram P</p><div class="flex gap-4 items-center"><a href="https://github.com/ajay-vikram" target="_blank" rel="noopener noreferrer" class="text-muted-foreground hover:text-foreground transition-colors" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="w-3.5 h-3.5" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://www.linkedin.com/in/ajay-vikram777/" target="_blank" rel="noopener noreferrer" class="text-muted-foreground hover:text-foreground transition-colors" aria-label="LinkedIn"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="w-3.5 h-3.5" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://medium.com/@ajayvikramp" target="_blank" rel="noopener noreferrer" class="text-muted-foreground hover:text-foreground transition-colors" aria-label="Medium"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="w-3.5 h-3.5" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 32v448h448V32H0zm372.2 106.1l-24 23c-2.1 1.6-3.1 4.2-2.7 6.7v169.3c-.4 2.6.6 5.2 2.7 6.7l23.5 23v5.1h-118V367l24.3-23.6c2.4-2.4 2.4-3.1 2.4-6.7V199.8l-67.6 171.6h-9.1L125 199.8v115c-.7 4.8 1 9.7 4.4 13.2l31.6 38.3v5.1H71.2v-5.1l31.6-38.3c3.4-3.5 4.9-8.4 4.1-13.2v-133c.4-3.7-1-7.3-3.8-9.8L75 138.1V133h87.3l67.4 148L289 133.1h83.2v5z"></path></svg></a></div></div></footer></div><script src="/_next/static/chunks/webpack-e696bf7940c5214b.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/7aacf04632d0f10b.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:I[7690,[],\"\"]\n6:I[5250,[\"250\",\"static/chunks/250-913a093e5b4dc537.js\",\"101\",\"static/chunks/app/projects/%5Bslug%5D/page-78f26962105b3138.js\"],\"\"]\n7:I[5613,[],\"\"]\n9:I[1778,[],\"\"]\na:I[9968,[\"699\",\"static/chunks/8e1d74a4-35a4d45ac4ea158f.js\",\"250\",\"static/chunks/250-913a093e5b4dc537.js\",\"288\",\"static/chunks/288-dab10db7057b0ce7.js\",\"185\",\"static/chunks/app/layout-2518eb680be61c66.js\"],\"ThemeProvider\"]\nb:I[4002,[\"699\",\"static/chunks/8e1d74a4-35a4d45ac4ea158f.js\",\"250\",\"static/chunks/250-913a093e5b4dc537.js\",\""])</script><script>self.__next_f.push([1,"288\",\"static/chunks/288-dab10db7057b0ce7.js\",\"185\",\"static/chunks/app/layout-2518eb680be61c66.js\"],\"Navbar\"]\nc:I[6112,[\"699\",\"static/chunks/8e1d74a4-35a4d45ac4ea158f.js\",\"250\",\"static/chunks/250-913a093e5b4dc537.js\",\"288\",\"static/chunks/288-dab10db7057b0ce7.js\",\"185\",\"static/chunks/app/layout-2518eb680be61c66.js\"],\"Footer\"]\ne:I[8955,[],\"\"]\n8:[\"slug\",\"HAR\",\"d\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7aacf04632d0f10b.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"3lAnUym1JHLrsEzhC_AGp\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/projects/HAR\",\"initialTree\":[\"\",{\"children\":[\"projects\",{\"children\":[[\"slug\",\"HAR\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"HAR\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"projects\",{\"children\":[[\"slug\",\"HAR\",\"d\"],{\"children\":[\"__PAGE__\",{},[\"$L5\",[\"$\",\"article\",null,{\"className\":\"container py-10 px-4 md:px-6 max-w-4xl mx-auto\",\"children\":[[\"$\",\"$L6\",null,{\"href\":\"/#projects\",\"className\":\"inline-flex items-center text-sm font-medium text-muted-foreground hover:text-primary mb-8 transition-colors\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-left mr-2 h-4 w-4\",\"children\":[[\"$\",\"path\",\"1l729n\",{\"d\":\"m12 19-7-7 7-7\"}],[\"$\",\"path\",\"x3x0zl\",{\"d\":\"M19 12H5\"}],\"$undefined\"]}],\"Back to Projects\"]}],[\"$\",\"div\",null,{\"className\":\"prose prose-slate dark:prose-invert max-w-none prose-img:rounded-xl prose-img:shadow-lg prose-headings:scroll-mt-20\",\"children\":[[\"$\",\"h1\",\"h1-0\",{\"children\":\"Ternarizing CNNs for Human Activity Recognition\"}],\"\\n\",[\"$\",\"h2\",\"h2-0\",{\"children\":\"Overview\"}],\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":\"This project focuses on compressing a CNN model for human activity recognition using ternarization and quantization techniques. The aim is to significantly reduce model size and computational complexity without sacrificing much accuracy. The baseline model, trained on 64×64 spectrograms derived from radar signals, is ternarized and quantized to allow deployment with integer-only inference on resource-constrained embedded platforms.\"}],\"\\n\",[\"$\",\"h2\",\"h2-1\",{\"children\":\"Dataset\"}],\"\\n\",[\"$\",\"p\",\"p-1\",{\"children\":[\"The model is trained on a private dataset, using spectrogram representations of radar signals, each sized \",[\"$\",\"code\",\"code-0\",{\"children\":\"64×64×1\"}],\". These spectrograms capture temporal-frequency features of radar reflections and are well-suited for classifying fine-grained human activities. The dataset includes 5 distinct activity classes - \",[\"$\",\"code\",\"code-1\",{\"children\":\"{jogging, jumping, situp, waving, other}\"}],\", and training was performed using 5-fold cross-validation to ensure robust generalization. The spectrograms look like:\"]}],\"\\n\",[\"$\",\"p\",\"p-2\",{\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/spectrograms.png\",\"alt\":\"Input Spectrograms\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],\"\\n\",[\"$\",\"h2\",\"h2-2\",{\"children\":\"Model Architecture\"}],\"\\n\",[\"$\",\"p\",\"p-3\",{\"children\":\"The CNN architecture is lightweight, consisting of:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"A 2D convolution layer\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Two depthwise separable convolutions for spatial-temporal filtering\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Fully connected output layer with 5 class logits\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"p-4\",{\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/CNN.png\",\"alt\":\"CNN\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"white\",\"padding\":\"10px\"}}]}]}],\"\\n\",[\"$\",\"p\",\"p-5\",{\"children\":\"The model had 3029 parameters in total and was designed with efficiency in mind, enabling rapid inference while maintaining high classification accuracy.\"}],\"\\n\",[\"$\",\"h2\",\"h2-3\",{\"children\":\"Ternarization \u0026 Quantization\"}],\"\\n\",[\"$\",\"p\",\"p-6\",{\"children\":\"Quantization is the process of reducing the precision of numbers in a model—such as weights, biases and activations—by representing them with fewer bits. This helps make machine learning models smaller, faster, and more efficient for deployment on devices with limited resources. Ternarization uses 2 bits to represent each weight, restricting values to three discrete levels.\"}],\"\\n\",[\"$\",\"p\",\"p-7\",{\"children\":[\"The weight ternarization step maps full-precision weights to one of three values: {−1, 0, +1}. To ensure the ternary weight networks perform well, it is required to minimize the Euclidian distance between the full precision weights W and the ternary-valued weights W' while including a non-negative scaling factor α. Ternarization reduces the model size by over 16× and enables efficient bitwise operations. A threshold-based approach was used to determine which weights are quantized to -1, 0 and 1. In addition, the activations and biases were quantized to 8-bit and 23-bit respectively. Quantization was done using uniform affine mapping with a learned scale and zero-point, allowing real values to be approximated as \",[\"$\",\"code\",\"code-0\",{\"children\":\"q = round(r/scale) + zero_point\"}],\". The images below illustrate the floating-point weights and biases on the left, and their ternarized and quantized counterparts on the right.\"]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"overflow-x-auto my-8\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",\"thead-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"th\",\"th-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Floating Point Weights\"}],[\"$\",\"th\",\"th-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":\"Ternarized Integer Weights\"}]]}]}],[\"$\",\"tbody\",\"tbody-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/fp32weights.png\",\"alt\":\"FP32\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"center\"},\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/ternaryweights.png\",\"alt\":\"Ternary\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}]]}]}]],\"className\":\"w-full border-collapse text-sm\"}]}],\"\\n\",[\"$\",\"p\",\"p-8\",{\"children\":[\"The model was trained using Quantization-Aware Training (QAT). Unlike post-training quantization, QAT simulates quantization effects during training itself, enabling the network to learn robust representations under low-bit constraints. During training, fake quantization modules simulate low-bit precision, while the actual parameters remain in float for gradient updates. The Straight-Through Estimator (STE) was used to approximate gradients through the non-differentiable \",[\"$\",\"code\",\"code-0\",{\"children\":\"torch.round\"}],\" quantization function.\"]}],\"\\n\",[\"$\",\"p\",\"p-9\",{\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/QAT.png\",\"alt\":\"QAT\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],\"\\n\",[\"$\",\"p\",\"p-10\",{\"children\":\"To optimize further for inference, pruning was done to achieve ~45% sparsity and batch normalization layers were folded into the preceding convolutional layers, combining their parameters with the conv weights and biases. This reduced runtime complexity without changing model behavior.\"}],\"\\n\",[\"$\",\"p\",\"p-11\",{\"children\":\"Finally, all operations—including convolutions, batch norm, activation, and pooling—were implemented using integer-only arithmetic. Dyadic scaling factors of the form (s = a ⁄ 2ᵇ), where a and b are integers, were used for efficient scaling via bit-shifts instead of division, allowing deployment on fixed-point hardware with no floating-point support. This full quantization pipeline enabled robust, low-latency inference with minimal accuracy degradation compared to the full-precision baseline.\"}],\"\\n\",[\"$\",\"p\",\"p-12\",{\"children\":[\"$\",\"span\",null,{\"className\":\"block my-8\",\"children\":[\"$\",\"img\",null,{\"src\":\"/assets/img/int_inference.png\",\"alt\":\"Integer-Only Inference\",\"className\":\"rounded-xl shadow-lg mx-auto max-w-full\",\"style\":{\"maxWidth\":\"100%\",\"height\":\"auto\",\"backgroundColor\":\"transparent\",\"padding\":\"0\"}}]}]}],\"\\n\",[\"$\",\"h2\",\"h2-4\",{\"children\":\"Results\"}],\"\\n\",[\"$\",\"p\",\"p-13\",{\"children\":\"The model was evaluated using 5-fold cross-validation. Results below demonstrate only a marginal drop in accuracy post-ternarization:\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"overflow-x-auto my-8\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",\"thead-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"th\",\"th-0\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"Fold\"}],[\"$\",\"th\",\"th-1\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"FP32 Accuracy (%)\"}],[\"$\",\"th\",\"th-2\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"Ternarized Accuracy (%)\"}]]}]}],[\"$\",\"tbody\",\"tbody-0\",{\"children\":[[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"1\"}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"99.27\"}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"98.19\"}]]}],[\"$\",\"tr\",\"tr-1\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"2\"}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"100.00\"}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"99.64\"}]]}],[\"$\",\"tr\",\"tr-2\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"3\"}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"99.27\"}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"98.91\"}]]}],[\"$\",\"tr\",\"tr-3\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"4\"}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"98.55\"}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"97.83\"}]]}],[\"$\",\"tr\",\"tr-4\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"5\"}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"100.00\"}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"left\"},\"children\":\"100.00\"}]]}],[\"$\",\"tr\",\"tr-5\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"style\":{\"textAlign\":\"left\"},\"children\":[\"$\",\"strong\",\"strong-0\",{\"children\":\"Avg\"}]}],[\"$\",\"td\",\"td-1\",{\"style\":{\"textAlign\":\"left\"},\"children\":[\"$\",\"strong\",\"strong-0\",{\"children\":\"99.42\"}]}],[\"$\",\"td\",\"td-2\",{\"style\":{\"textAlign\":\"left\"},\"children\":[\"$\",\"strong\",\"strong-0\",{\"children\":\"98.91\"}]}]]}]]}]],\"className\":\"w-full border-collapse text-sm\"}]}],\"\\n\",[\"$\",\"p\",\"p-14\",{\"children\":\"Despite ~45% sparsity and ternarized weights, the accuracy remained close to baseline, validating the effectiveness of ternarization and quantization for embedded HAR systems.\"}]]}]]}],null]]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"projects\",\"children\",\"$8\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"projects\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-screen bg-background font-sans antialiased __className_f367f3\",\"children\":[\"$\",\"$La\",null,{\"attribute\":\"class\",\"forcedTheme\":\"dark\",\"disableTransitionOnChange\":true,\"children\":[\"$\",\"div\",null,{\"className\":\"relative flex min-h-screen flex-col\",\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"$Lc\",null,{}]]}]}]}]}],null]],\"initialHead\":[false,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Ajay Vikram P - Machine Learning Researcher\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Portfolio of Ajay Vikram P, a Machine Learning Researcher focused on deep learning and efficient model design.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>