"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[288],{288:function(e,n,i){i.d(n,{M8:function(){return t},Po:function(){return o},b8:function(){return a},f3:function(){return s},nb:function(){return r},q:function(){return l}});let t={name:"Ajay Vikram P",role:"Machine Learning Researcher and Engineer",bio:"Computer Science Master’s student at Duke University focused on Deep Learning and Model Optimization. I have built specialized pipelines for autonomous systems, neuromorphic recognition and tracking, VLM reasoning, object detection, model compression and biomedical analysis. My diverse background spans academic research and industry roles, driving improvements in both model architecture and practical performance.",email:"ajayvikramp@gmail.com",location:"Durham, North Carolina, US",github:"https://github.com/ajay-vikram",linkedin:"https://www.linkedin.com/in/ajay-vikram777/",medium:"https://medium.com/@ajayvikramp",image:"/assets/img/main.jpg",formspreeId:"xjvnaeeq"},a=[{role:"Graduate Research Assistant",company:"Duke University",location:"Durham, NC",period:"Aug 2025 - Present",advisor:"Andrew Michael",advisorLink:"https://dibs.duke.edu/profile/andrew-michael-phd/",description:""},{role:"ML Research Intern",company:"Indian Institute of Science",location:"Bengaluru, India",period:"May 2024 - July 2025",advisor:"Chetan Singh Thakur",advisorLink:"https://labs.dese.iisc.ac.in/neuronics/people/",description:""},{role:"Machine Learning Intern",company:"Lamarr",location:"Remote",period:"Mar 2024 - Apr 2024",description:""},{role:"Software Intern",company:"Qualcomm",location:"Hyderabad, India",period:"May 2023 - July 2023",description:""}],s=[{school:"Duke University",degree:"M.S. in Computer Science (AI/ML)",period:"2025 - Present"},{school:"National Institute of Technology Karnataka",degree:"B.Tech in Computer Science",period:"2020 - 2024"}],r=["Python","C/C++","PyTorch","TensorFlow","Deep Learning","CV","NLP","Neuromorphic Vision","Quantization","MLOps","Docker","Kubernetes","Jenkins","MLflow","MySQL","ROS2","HTML/CSS","Git"],o=[{title:"HOMI: Ultra-Fast EdgeAI platform for Event Cameras",conference:"Preprint",authors:"Shankaranarayanan H, Satyapreet Singh Yadav, Adithya Krishna, Ajay Vikram P, Mahesh Mehendale, Chetan Singh Thakur",link:"https://arxiv.org/abs/2508.12637"},{title:"ViTFuser: Advancements in Global Context Understanding for Autonomous Vehicles",conference:"6th International Conference on Machine Learning, Image Processing, Network Security and Data Sciences (MIND 2024)",authors:"Atanu Chatterjee, Ajay Vikram P, Aniketh Narayan Bellala, T Naga Tarun, Basavaraj Talawar, Vani M, and Jeny Rajan"}],l=[{slug:"SkillBlenderLC",title:"SkillBlenderLC",subtitle:"Self-Evolving Meta-Skill Learning for Humanoid Robots",description:"Language-conditioned extension of SkillBlender enabling autonomous skill discovery and reward learning for humanoids using LLMs.",image:"/assets/img/robot.png",content:'\n# SkillBlenderLC: Self-Evolving Meta-Skill Learning for Humanoid Robots\n\n## Overview\n\nSkillBlenderLC is a language-conditioned extension of the [SkillBlender](https://github.com/Humanoid-SkillBlender/SkillBlender) framework that enables autonomous skill discovery and reward learning for humanoid robots. The system leverages Large Language Models (LLMs) to decompose long-horizon tasks, identify missing skills, automatically generate reward functions, and iteratively refine them using reinforcement learning feedback.\n\n![System Overview](/assets/img/plan.png)\n\n---\n\n## Motivation\n\n- Long-horizon humanoid tasks require sequencing and composing multiple motion skills.\n- Fixed skill libraries limit scalability and generalization.\n- Manual reward design is brittle, time-consuming, and non-scalable.\n- Existing LLM-based reward methods do not adapt based on downstream RL performance.\n\nSkillBlenderLC addresses these challenges with a self-evolving learning loop that connects language reasoning directly to control learning.\n\n---\n\n## System Design\n\nSkillBlenderLC augments the original SkillBlender training stack with three core components:\n\n### 1. Task Decomposition\nAn LLM decomposes a natural-language task into ordered subtasks and required skills, proposing new primitive skills when the current library is insufficient.\n\n### 2. Automatic Reward Bootstrapping\nFor missing skills, the system synthesizes executable Python reward functions using:\n- Existing reward templates\n- Humanoid environment dynamics\n\n### 3. Reward Refinement via RL Feedback\nAfter RL training, performance metrics (stability, convergence, success rate) are fed back to the LLM to refine the reward, enabling iterative improvement and self-evolution.\n\n---\n\n## Experimental Setup\n\n- **Simulator**: NVIDIA Isaac Gym (PhysX)\n- **Robot**: Unitree H1 (19-DoF humanoid)\n- **RL Algorithm**: PPO\n- **Parallel Environments**: 4,096\n- **Training**: 15,000 episodes per skill\n\n---\n\n## LLMs Evaluated\n\nWe benchmarked six state-of-the-art LLMs across task decomposition and reward generation:\n\n- GPT-4o\n- Claude Sonnet 4.5\n- Gemini 2.5 Flash\n- LLaMA 3.3 70B\n- Mistral Large\n- DeepSeek V3\n\n---\n\n## Results\n\n### Skill Selection on Long-Horizon Tasks\n\nEvaluated on the eight long-horizon tasks from SkillBlender:\n\n![Skill Selection Results](/assets/img/skill_selection_results.png)\n\n**Key insight**:\nLLMs reliably identify primary motion primitives but often miss secondary or contact-rich skills, motivating downstream reward learning.\n\n---\n\n### Reward Generation — Primitive Skill: Squatting\n\nPerformance of policies trained using LLM-generated rewards for the Squatting primitive:\n\n| Ground Truth | GPT-4o | Claude 4.5 | Gemini |\n| :---: | :---: | :---: | :---: |\n| ![GT](/assets/img/squat_gt.gif) | ![GPT4o](/assets/img/squat_gpt4o.gif) | ![Sonnet](/assets/img/squat_sonnet.gif) | ![Gemini](/assets/img/squat_gemini.gif) |\n\n| LLaMA | DeepSeek | Mistral |\n| :---: | :---: | :---: |\n| ![LLaMA](/assets/img/squat_llama.gif) | ![DeepSeek](/assets/img/squat_deepseek.gif) | ![Mistral](/assets/img/squat_mistral.gif) |\n\n\n**Observation**:\nWhile most models reproduce reward structure correctly, numerical misspecification often leads to unstable or ineffective policies.\n\n---\n\n### Reward Generation — Combined Skill: Button Press\n\nThe Button Press task requires composing walking and reaching:\n\n| Ground Truth | LLM Generated |\n| :---: | :---: |\n| ![GT](/assets/img/buttonpressgt.gif) | ![LLM](/assets/img/buttonpress.gif) |\n\n**Result**:\nOnly Claude-generated rewards successfully enable task completion.\nMost models produce rewards that cause the robot to remain stationary, highlighting the difficulty of multi-stage skill composition.\n\n---\n\n### Learning New Skills\n\nThe framework can generate and train rewards for previously unseen skills:\n\n| Bend | Sidestep | Kick |\n| :---: | :---: | :---: |\n| ![Bend](/assets/img/bend_back.gif) | ![Sidestep](/assets/img/sidestep.gif) | ![Kick](/assets/img/kick.gif) |\n\n**Insight**:\nSome skills are successfully learned, while others require iterative refinement — validating the need for a closed-loop reward evolution mechanism.\n\n---\n\n## Prompt Design\n\nWe design explicit, constrained prompts for each stage to ensure consistency, interpretability, and reproducibility.\n\n### Task Decomposition Prompt\n\n```text\nYou are a high-level humanoid motion planner and meta-skill developer.\n\nYour task is to decompose a long-horizon natural-language goal into a sequence of\nshort, atomic subtasks. Each subtask must include a concise "subgoal" (one line of natural language)\nand a list of "skills" required to achieve it.\n\nYou currently have access to the following SKILL LIBRARY:\n\n{SKILL_LIBRARY}\n\nFor every subgoal:\n1. Compare the subgoal against the skills in the current library.\n2. If ALL required abilities exist, reuse them directly.\n3. If the subgoal requires a new ability not in the list, invent a new skill:\n   - Use short (1–2 words), descriptive, physically plausible humanoid actions\n   - Avoid abstract or cognitive terms\n   - Do not use synonyms of existing skills unless functionality differs\n   - If no existing skill fully covers the subgoal, you must create a new primitive skill\n\nThe final output must be a valid JSON object of the form:\n\n{\n  "subtasks": [\n    {"subgoal": "<short actionable goal>", "skills": ["Skill1", "Skill2", ...]},\n    ...\n  ]\n}\n\nGuidelines:\n- Ensure subtasks are sequential and fully cover the task\n- Keep skill names capitalized and concise\n- Keep subgoals to one line\n- Return only the JSON with no explanations or comments\n\n```\n\n### Reward Generation Prompt\n\n```text\nYou are a code-generation agent designing a new humanoid skill reward function.\nReturn only valid Python code defining the reward class.\nDo not include explanations, markdown fences, comments, or introductory text.\n\nBelow are the existing primitive skill rewards currently used:\n<existing_rewards>\n{reward_context}\n</existing_rewards>\n\nAnalyze these to understand parameter conventions and scaling.\n\nBelow is the humanoid environment definition used for training:\n<environment_context>\n{env_context}\n</environment_context>\n\nNow, design a NEW reward class for the following subgoal and skill:\n\nSubgoal: {subgoal}\nSkill: {skill_name}\n\nRules:\n1. Maintain the same Python class structure (class rewards: with nested class scales:)\n2. Reuse ONLY variables, parameters, and scale names already defined\n3. Do NOT invent new variables or physics metrics\n4. Keep numerical scales realistic and consistent with reference rewards\n5. Return only valid Python code, with no explanations or comments\n```\n\n### Reward Refinement Prompt\n\n```text\nYou are a humanoid reward designer.\nThe following reward function was trained in RL but performed poorly.\n\nSubgoal: {subgoal}\nSkill: {skill}\n\n[Generated reward]\n{generated_reward}\n\n[Observed RL feedback metrics]\n{feedback_str}\n\nBased on these feedback metrics, rewrite the reward function to:\n1. Improve stability, convergence, and success rate\n2. Keep structure consistent with existing reward format (class rewards: with nested scales)\n3. Keep parameter names consistent\n4. Adjust only scale magnitudes or add missing shaping terms if necessary\n\nOutput only the corrected Python code starting with \'class rewards:\'.\nDo NOT include explanations or markdown.\n```\n    '},{slug:"ViTFuser",title:"ViTFuser",subtitle:"Global Context Understanding for Autonomous Vehicles",description:"An attention-based deep learning architecture introducing multi-stage Vision Transformers and Feature Pyramid Networks to improve autonomous vehicle decision-making.",image:"/assets/img/carla.png",content:"\n# ViTFuser: Advancements in Global Context Understanding for Autonomous Vehicles\n\n## Overview\n\nViTFuser is an attention-based deep learning architecture designed to improve decision-making in autonomous vehicles. It addresses limitations in prior state-of-the-art models (like [TransFuser](https://arxiv.org/pdf/2205.15997)) that suffer from loss of global context during feature fusion. ViTFuser introduces multi-stage Vision Transformers (ViTs) and a Feature Pyramid Network (FPN) to better understand the scene and reduce traffic violations.\n\nThe project outperforms comparative existing models on CARLA simulation benchmarks, achieving higher Driving Scores (DS) and lower traffic infractions.\n\n![High Level Overview](/assets/img/high_level.png)\n\n---\n\n## Dataset & Task\n\nViTFuser was evaluated on the CARLA 0.9.10 simulator, using data recorded at 2 FPS across 3,500 driving routes in varying towns and weather conditions.\n\n- **Input sensors**:\n  - RGB images from 3 front-facing cameras (stitched into a wide-view image)\n  - LiDAR point cloud converted to 2D Bird’s-Eye-View (BEV) grid\n- **Output**: 4 future waypoints for the ego vehicle\n\n![Input Modality](/assets/img/input_modality.png)\n\nThe goal is to predict waypoints accurately while minimizing infractions during navigation.\n\n---\n\n## Model Architecture\n\nThe model consists of two main components:\n\n### 1. **Perception Module (Encoder)**\n- Processes RGB and LiDAR inputs using CNNs + multi-resolution Vision Transformers (ViT)\n- RGB and LiDAR branches run in parallel, each extracting hierarchical features\n- Cross-modal attention enables global context fusion\n- Uses Feature Pyramid Network (FPN) for multi-scale feature extraction, boosting object detection accuracy\n\n![Encoder](/assets/img/encoder.png)\n\n### 2. **Decision Module (Decoder)**\n- Predicts vehicle waypoints using a GRU-based network\n- Handles auxiliary tasks such as:\n  - Semantic segmentation\n  - HD map generation\n  - Depth estimation\n  - Object detection (bounding boxes)\n\nThese tasks improve interpretability and overall navigation performance.\n\n---\n\n## Parameters Comparison\n\n| Model | Parameters |\n| :--- | :--- |\n| TransFuser | 168 million |\n| Swin Transformer | 688 million |\n| **ViTFuser** | **55 million** |\n\n> ViTFuser reduces parameters by **~67%** compared to TransFuser and **~91%** compared to Swin Transformer, making it much more efficient while maintaining strong performance.\n\n---\n\n## Results\n\nViTFuser was benchmarked on **Longest6** and **Town05** datasets from CARLA.\n\n### Longest6 Benchmark\n\n| Model | DS | RC | IS |\n| :--- | :--- | :--- | :--- |\n| TransFuser | 43.48 | 77.72 | 0.60 |\n| ViTFuser | 51.96 | 80.70 | 0.65 |\n| **ViTFuser + FPN** | **55.15** | **81.43** | **0.69** |\n\n### Town05 Benchmark\n\n| Model | DS (Short) | RC (Short) | DS (Long) | RC (Long) |\n| :--- | :--- | :--- | :--- | :--- |\n| TransFuser | 87.48 | 92.78 | 67.85 | 91.57 |\n| ViTFuser | 90.04 | 94.79 | 74.82 | 92.40 |\n| **ViTFuser + FPN** | **91.07** | **94.67** | **74.95** | **94.90** |\n\nDS - Driving Score, RC - Route Completion, IS - Infraction Score\n    "},{slug:"HAR",title:"Ternarizing CNNs",subtitle:"Efficient Human Activity Recognition",description:"Compressing a CNN model for human activity recognition using ternarization and quantization techniques for efficient deployment.",image:"/assets/img/har.png",content:"\n# Ternarizing CNNs for Human Activity Recognition\n\n## Overview\n\nThis project focuses on compressing a CNN model for human activity recognition using ternarization and quantization techniques. The aim is to significantly reduce model size and computational complexity without sacrificing much accuracy. The baseline model, trained on 64\xd764 spectrograms derived from radar signals, is ternarized and quantized to allow deployment with integer-only inference on resource-constrained embedded platforms.\n\n## Dataset\n\nThe model is trained on a private dataset, using spectrogram representations of radar signals, each sized `64\xd764\xd71`. These spectrograms capture temporal-frequency features of radar reflections and are well-suited for classifying fine-grained human activities. The dataset includes 5 distinct activity classes - `{jogging, jumping, situp, waving, other}`, and training was performed using 5-fold cross-validation to ensure robust generalization. The spectrograms look like:\n\n![Input Spectrograms](/assets/img/spectrograms.png)\n\n## Model Architecture\n\nThe CNN architecture is lightweight, consisting of:\n- A 2D convolution layer\n- Two depthwise separable convolutions for spatial-temporal filtering\n- Fully connected output layer with 5 class logits\n\n![CNN](/assets/img/CNN.png)\n\nThe model had 3029 parameters in total and was designed with efficiency in mind, enabling rapid inference while maintaining high classification accuracy.\n\n## Ternarization & Quantization\n\nQuantization is the process of reducing the precision of numbers in a model—such as weights, biases and activations—by representing them with fewer bits. This helps make machine learning models smaller, faster, and more efficient for deployment on devices with limited resources. Ternarization uses 2 bits to represent each weight, restricting values to three discrete levels.\n\nThe weight ternarization step maps full-precision weights to one of three values: {−1, 0, +1}. To ensure the ternary weight networks perform well, it is required to minimize the Euclidian distance between the full precision weights W and the ternary-valued weights W' while including a non-negative scaling factor α. Ternarization reduces the model size by over 16\xd7 and enables efficient bitwise operations. A threshold-based approach was used to determine which weights are quantized to -1, 0 and 1. In addition, the activations and biases were quantized to 8-bit and 23-bit respectively. Quantization was done using uniform affine mapping with a learned scale and zero-point, allowing real values to be approximated as `q = round(r/scale) + zero_point`. The images below illustrate the floating-point weights and biases on the left, and their ternarized and quantized counterparts on the right.\n\n| Floating Point Weights | Ternarized Integer Weights |\n| :---: | :---: |\n| ![FP32](/assets/img/fp32weights.png) | ![Ternary](/assets/img/ternaryweights.png) |\n\nThe model was trained using Quantization-Aware Training (QAT). Unlike post-training quantization, QAT simulates quantization effects during training itself, enabling the network to learn robust representations under low-bit constraints. During training, fake quantization modules simulate low-bit precision, while the actual parameters remain in float for gradient updates. The Straight-Through Estimator (STE) was used to approximate gradients through the non-differentiable `torch.round` quantization function.\n\n![QAT](/assets/img/QAT.png)\n\nTo optimize further for inference, pruning was done to achieve ~45% sparsity and batch normalization layers were folded into the preceding convolutional layers, combining their parameters with the conv weights and biases. This reduced runtime complexity without changing model behavior.\n\nFinally, all operations—including convolutions, batch norm, activation, and pooling—were implemented using integer-only arithmetic. Dyadic scaling factors of the form (s = a ⁄ 2ᵇ), where a and b are integers, were used for efficient scaling via bit-shifts instead of division, allowing deployment on fixed-point hardware with no floating-point support. This full quantization pipeline enabled robust, low-latency inference with minimal accuracy degradation compared to the full-precision baseline.\n\n![Integer-Only Inference](/assets/img/int_inference.png)\n\n## Results\n\nThe model was evaluated using 5-fold cross-validation. Results below demonstrate only a marginal drop in accuracy post-ternarization:\n\n| Fold | FP32 Accuracy (%) | Ternarized Accuracy (%) |\n| :--- | :--- | :--- |\n| 1 | 99.27 | 98.19 |\n| 2 | 100.00 | 99.64 |\n| 3 | 99.27 | 98.91 |\n| 4 | 98.55 | 97.83 |\n| 5 | 100.00 | 100.00 |\n| **Avg** | **99.42** | **98.91** |\n\nDespite ~45% sparsity and ternarized weights, the accuracy remained close to baseline, validating the effectiveness of ternarization and quantization for embedded HAR systems.\n    "},{slug:"BCI",title:"Hand Kinematics in BCI",subtitle:"State-Space Models (S4, LMU) for Decoding",description:"Decoding continuous hand movements from brain activity using State-Space Models (S4, LMU) to predict 2D hand position.",image:"/assets/img/BCI.png",content:"\n# Decoding Hand Kinematics in Brain-Computer Interfaces with State-Space Models\n\n## Overview\nThis project focused on decoding continuous hand movements from brain activity. While most BCI models classify discrete movement intentions, we tackled the more complex task of predicting **continuous hand trajectories** — essential for fine motor control in real-world applications.\n\n---\n\n## Dataset & Task\nWe used the [Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology](https://zenodo.org/records/583331) dataset, which contains spike recordings from two macaques (Indy and Loco) reaching for targets arranged in an 8\xd78 grid.\n\n- **96–192 channels** from motor and somatosensory cortices\n- **No pre-movement delay**; free-paced reaching\n- **Predict 2D hand position** using only the last 50 timesteps of spiking activity\n\n*Note:* The above link is for reference, the actual loading and processing are done using the [neurobench](https://github.com/NeuroBench/neurobench) code harness\n\n---\n\n## Models Trained\n\nWe trained 2 State-Space Models on this dataset:\n1. [Legendre Memory Units (LMUs)](https://proceedings.neurips.cc/paper_files/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf)\n2. [Structured State Space Sequence Models (S4)](https://arxiv.org/pdf/2111.00396)\n\nRefer to my [blog post](https://medium.com/@ajayvikramp/ssms-for-efficient-long-range-memory-d6a78226c738) on State-Space Models if you are new to the topic.\n\n---\n\n## Model Architecture\n\n- The LMU was trained with hidden size 16 and memory size 32\n- The S4 was trained with hidden size 64 and memory size 64\n\n*Note:* The models were trained with a sub window binning method on the spikes.\n\n---\n\n## Results\n\n| Model | Parameters | Test R2 Score |\n| :--- | :--- | :--- |\n| ANN 2D | 5K | 0.62 |\n| ANN 3D | 24K | 0.65 |\n| LSTM | 44K | 0.58 |\n| EEGNet | 11K | 0.56 |\n| **LMU** | **7K** | **0.70** |\n| **S4** | **70K** | **0.75** |\n\n*The results shown above are for the 3rd session of Monkey 1\n    "}]}}]);